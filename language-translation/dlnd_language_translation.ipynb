{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Language Translation\n",
    "In this project, you’re going to take a peek into the realm of neural network machine translation.  You’ll be training a sequence to sequence model on a dataset of English and French sentences that can translate new sentences from English to French.\n",
    "## Get the Data\n",
    "Since translating the whole language of English to French will take lots of time to train, we have provided you with a small portion of the English corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "source_path = 'data/small_vocab_en'\n",
    "target_path = 'data/small_vocab_fr'\n",
    "source_text = helper.load_data(source_path)\n",
    "target_text = helper.load_data(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "Play around with view_sentence_range to view different parts of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 227\n",
      "Number of sentences: 137861\n",
      "Average number of words in a sentence: 13.225277634719028\n",
      "\n",
      "English sentences 0 to 10:\n",
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "the united states is usually chilly during july , and it is usually freezing in november .\n",
      "california is usually quiet during march , and it is usually hot in june .\n",
      "the united states is sometimes mild during june , and it is cold in september .\n",
      "your least liked fruit is the grape , but my least liked is the apple .\n",
      "his favorite fruit is the orange , but my favorite is the grape .\n",
      "paris is relaxing during december , but it is usually chilly in july .\n",
      "new jersey is busy during spring , and it is never hot in march .\n",
      "our least liked fruit is the lemon , but my least liked is the grape .\n",
      "the united states is sometimes busy during january , and it is sometimes warm in november .\n",
      "\n",
      "French sentences 0 to 10:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
      "votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
      "son fruit préféré est l'orange , mais mon préféré est le raisin .\n",
      "paris est relaxant en décembre , mais il est généralement froid en juillet .\n",
      "new jersey est occupé au printemps , et il est jamais chaude en mars .\n",
      "notre fruit est moins aimé le citron , mais mon moins aimé est le raisin .\n",
      "les états-unis est parfois occupé en janvier , et il est parfois chaud en novembre .\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in source_text.split()})))\n",
    "\n",
    "sentences = source_text.split('\\n')\n",
    "word_counts = [len(sentence.split()) for sentence in sentences]\n",
    "print('Number of sentences: {}'.format(len(sentences)))\n",
    "print('Average number of words in a sentence: {}'.format(np.average(word_counts)))\n",
    "\n",
    "print()\n",
    "print('English sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(source_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))\n",
    "print()\n",
    "print('French sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(target_text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocessing Function\n",
    "### Text to Word Ids\n",
    "As you did with other RNNs, you must turn the text into a number so the computer can understand it. In the function `text_to_ids()`, you'll turn `source_text` and `target_text` from words to ids.  However, you need to add the `<EOS>` word id at the end of each sentence from `target_text`.  This will help the neural network predict when the sentence should end.\n",
    "\n",
    "You can get the `<EOS>` word id by doing:\n",
    "```python\n",
    "target_vocab_to_int['<EOS>']\n",
    "```\n",
    "You can get other word ids using `source_vocab_to_int` and `target_vocab_to_int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert source and target text to proper word ids\n",
    "    :param source_text: String that contains all the source text.\n",
    "    :param target_text: String that contains all the target text.\n",
    "    :param source_vocab_to_int: Dictionary to go from the source words to an id\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: A tuple of lists (source_id_text, target_id_text)\n",
    "    \"\"\"\n",
    "    # Split text into sentences\n",
    "    source_text_lst = source_text.split('\\n')\n",
    "    target_text_lst = target_text.split('\\n')\n",
    "    # Append <EOS> at the end of each sententence\n",
    "    target_text_lst = [sentence + ' <EOS>' for sentence in target_text_lst]\n",
    "    # Make lists using vocab to int mapping\n",
    "    source_id_text = [[source_vocab_to_int[word] for word in sentence.split()] for sentence in source_text_lst]\n",
    "    target_id_text = [[target_vocab_to_int[word] for word in sentence.split()] for sentence in target_text_lst]\n",
    "\n",
    "    return source_id_text, target_id_text\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_text_to_ids(text_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "helper.preprocess_and_save_data(source_path, target_path, text_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import helper\n",
    "\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Check the Version of TensorFlow and Access to GPU\n",
    "This will check to make sure you have the correct version of TensorFlow and access to a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) in [LooseVersion('1.0.0'), LooseVersion('1.0.1')], 'This project requires TensorFlow version 1.0  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the Neural Network\n",
    "You'll build the components necessary to build a Sequence-to-Sequence model by implementing the following functions below:\n",
    "- `model_inputs`\n",
    "- `process_decoding_input`\n",
    "- `encoding_layer`\n",
    "- `decoding_layer_train`\n",
    "- `decoding_layer_infer`\n",
    "- `decoding_layer`\n",
    "- `seq2seq_model`\n",
    "\n",
    "### Input\n",
    "Implement the `model_inputs()` function to create TF Placeholders for the Neural Network. It should create the following placeholders:\n",
    "\n",
    "- Input text placeholder named \"input\" using the TF Placeholder name parameter with rank 2.\n",
    "- Targets placeholder with rank 2.\n",
    "- Learning rate placeholder with rank 0.\n",
    "- Keep probability placeholder named \"keep_prob\" using the TF Placeholder name parameter with rank 0.\n",
    "\n",
    "Return the placeholders in the following the tuple (Input, Targets, Learing Rate, Keep Probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate, keep probability)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name ='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name = 'labels')\n",
    "    lr = tf.placeholder(tf.float32, None, name='learning')\n",
    "    kp = tf.placeholder(tf.float32, None, name='keep_prob')\n",
    "    return inputs, targets, lr, kp\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_model_inputs(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Process Decoding Input\n",
    "Implement `process_decoding_input` using TensorFlow to remove the last word id from each batch in `target_data` and concat the GO ID to the begining of each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def process_decoding_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for dencoding\n",
    "    :param target_data: Target Placehoder\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param batch_size: Batch Size\n",
    "    :return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], target_vocab_to_int['<GO>']), ending], 1)\n",
    "    return dec_input\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_process_decoding_input(process_decoding_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Encoding\n",
    "Implement `encoding_layer()` to create a Encoder RNN layer using [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob):\n",
    "    \"\"\"\n",
    "    Create encoding layer\n",
    "    :param rnn_inputs: Inputs for the RNN\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: RNN state\n",
    "    \"\"\"\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    \n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n",
    "    \n",
    "    output, enc_state = tf.nn.dynamic_rnn(cell, rnn_inputs, dtype=tf.float32)\n",
    "    \n",
    "    return enc_state\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_encoding_layer(encoding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Decoding - Training\n",
    "Create training logits using [`tf.contrib.seq2seq.simple_decoder_fn_train()`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/simple_decoder_fn_train) and [`tf.contrib.seq2seq.dynamic_rnn_decoder()`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder).  Apply the `output_fn` to the [`tf.contrib.seq2seq.dynamic_rnn_decoder()`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder) outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope,\n",
    "                         output_fn, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for training\n",
    "    :param encoder_state: Encoder State\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embed_input: Decoder embedded input\n",
    "    :param sequence_length: Sequence Length\n",
    "    :param decoding_scope: TenorFlow Variable Scope for decoding\n",
    "    :param output_fn: Function to apply the output layer\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: Train Logits\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    cell = tf.contrib.seq2seq.simple_decoder_fn_train(encoder_state)\n",
    "    \n",
    "    outputs, final_state, final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(\n",
    "        dec_cell, cell, dec_embed_input, sequence_length, scope=decoding_scope)\n",
    "    \n",
    "    output = output_fn(outputs)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_decoding_layer_train(decoding_layer_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Decoding - Inference\n",
    "Create inference logits using [`tf.contrib.seq2seq.simple_decoder_fn_inference()`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/simple_decoder_fn_inference) and [`tf.contrib.seq2seq.dynamic_rnn_decoder()`](https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id,\n",
    "                         maximum_length, vocab_size, decoding_scope, output_fn, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for inference\n",
    "    :param encoder_state: Encoder state\n",
    "    :param dec_cell: Decoder RNN Cell\n",
    "    :param dec_embeddings: Decoder embeddings\n",
    "    :param start_of_sequence_id: GO ID\n",
    "    :param end_of_sequence_id: EOS Id\n",
    "    :param maximum_length: The maximum allowed time steps to decode\n",
    "    :param vocab_size: Size of vocabulary\n",
    "    :param decoding_scope: TensorFlow Variable Scope for decoding\n",
    "    :param output_fn: Function to apply the output layer\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: Inference Logits\n",
    "    \"\"\"\n",
    "    \n",
    "    infer_decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_inference(\n",
    "        output_fn, encoder_state, dec_embeddings, start_of_sequence_id, end_of_sequence_id, \n",
    "        maximum_length, vocab_size)\n",
    "    \n",
    "    inference_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, infer_decoder_fn, scope=decoding_scope)\n",
    "    \n",
    "    return inference_logits\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_decoding_layer_infer(decoding_layer_infer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the Decoding Layer\n",
    "Implement `decoding_layer()` to create a Decoder RNN layer.\n",
    "\n",
    "- Create RNN cell for decoding using `rnn_size` and `num_layers`.\n",
    "- Create the output fuction using [`lambda`](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions) to transform it's input, logits, to class logits.\n",
    "- Use the your `decoding_layer_train(encoder_state, dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob)` function to get the training logits.\n",
    "- Use your `decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id, maximum_length, vocab_size, decoding_scope, output_fn, keep_prob)` function to get the inference logits.\n",
    "\n",
    "Note: You'll need to use [tf.variable_scope](https://www.tensorflow.org/api_docs/python/tf/variable_scope) to share variables between training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size,\n",
    "                   num_layers, target_vocab_to_int, keep_prob):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :param dec_embed_input: Decoder embedded input\n",
    "    :param dec_embeddings: Decoder embeddings\n",
    "    :param encoder_state: The encoded state\n",
    "    :param vocab_size: Size of vocabulary\n",
    "    :param sequence_length: Sequence Length\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :param keep_prob: Dropout keep probability\n",
    "    :return: Tuple of (Training Logits, Inference Logits)\n",
    "    \"\"\"\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "  \n",
    "    maximum_length = sequence_length\n",
    "\n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        \n",
    "        output_fn = lambda x: tf.contrib.layers.fully_connected(x, \n",
    "                                                                vocab_size, \n",
    "                                                                None, \n",
    "                                                                scope=decoding_scope\n",
    "                                                               )\n",
    "        \n",
    "        train_logits = decoding_layer_train(encoder_state,\n",
    "                                            dec_cell,\n",
    "                                            dec_embed_input,\n",
    "                                            sequence_length,\n",
    "                                            decoding_scope,\n",
    "                                            output_fn,\n",
    "                                            keep_prob\n",
    "                                           )\n",
    "        decoding_scope.reuse_variables()\n",
    "        \n",
    "        start_id = target_vocab_to_int['<GO>']\n",
    "        end_id = target_vocab_to_int[\"<EOS>\"]\n",
    "        \n",
    "        inference_logits = decoding_layer_infer(encoder_state, \n",
    "                                                dec_cell, \n",
    "                                                dec_embeddings, \n",
    "                                                start_id, \n",
    "                                                end_id,\n",
    "                                                maximum_length, \n",
    "                                                vocab_size, \n",
    "                                                decoding_scope, \n",
    "                                                output_fn, \n",
    "                                                keep_prob\n",
    "                                               )\n",
    "        \n",
    "    return train_logits, inference_logits\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_decoding_layer(decoding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "\n",
    "- Apply embedding to the input data for the encoder.\n",
    "- Encode the input using your `encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob)`.\n",
    "- Process target data using your `process_decoding_input(target_data, target_vocab_to_int, batch_size)` function.\n",
    "- Apply embedding to the target data for the decoder.\n",
    "- Decode the encoded input using your `decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size, num_layers, target_vocab_to_int, keep_prob)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size, sequence_length, source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size, rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence part of the neural network\n",
    "    :param input_data: Input placeholder\n",
    "    :param target_data: Target placeholder\n",
    "    :param keep_prob: Dropout keep probability placeholder\n",
    "    :param batch_size: Batch Size\n",
    "    :param sequence_length: Sequence Length\n",
    "    :param source_vocab_size: Source vocabulary size\n",
    "    :param target_vocab_size: Target vocabulary size\n",
    "    :param enc_embedding_size: Decoder embedding size\n",
    "    :param dec_embedding_size: Encoder embedding size\n",
    "    :param rnn_size: RNN Size\n",
    "    :param num_layers: Number of layers\n",
    "    :param target_vocab_to_int: Dictionary to go from the target words to an id\n",
    "    :return: Tuple of (Training Logits, Inference Logits)\n",
    "    \"\"\"\n",
    "    enc_embeddings = tf.Variable(tf.random_uniform([source_vocab_size, enc_embedding_size]))\n",
    "    enc_embed_input = tf.nn.embedding_lookup(enc_embeddings, input_data)\n",
    "\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, dec_embedding_size]))\n",
    "    dec_input = process_decoding_input(target_data, target_vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    \n",
    "    enc_state = encoding_layer(enc_embed_input,rnn_size,num_layers,keep_prob)\n",
    "    \n",
    "\n",
    "    return decoding_layer(dec_embed_input,dec_embeddings,enc_state,target_vocab_size,sequence_length,\n",
    "                             rnn_size, num_layers, target_vocab_to_int, keep_prob)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_seq2seq_model(seq2seq_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `num_layers` to the number of layers.\n",
    "- Set `encoding_embedding_size` to the size of the embedding for the encoder.\n",
    "- Set `decoding_embedding_size` to the size of the embedding for the decoder.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `keep_probability` to the Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "epochs = 100\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 50\n",
    "# Number of Layers\n",
    "num_layers = 3\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 15\n",
    "decoding_embedding_size = 15\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Dropout Keep Probability\n",
    "keep_probability = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_path = 'checkpoints/dev'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = helper.load_preprocess()\n",
    "max_source_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, lr, keep_prob = model_inputs()\n",
    "    sequence_length = tf.placeholder_with_default(max_source_sentence_length, None, name='sequence_length')\n",
    "    input_shape = tf.shape(input_data)\n",
    "    \n",
    "    train_logits, inference_logits = seq2seq_model(\n",
    "        tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(source_vocab_to_int), len(target_vocab_to_int),\n",
    "        encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, target_vocab_to_int)\n",
    "\n",
    "    tf.identity(inference_logits, 'logits')\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            train_logits,\n",
    "            targets,\n",
    "            tf.ones([input_shape[0], sequence_length]))\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train\n",
    "Train the neural network on the preprocessed data. If you have a hard time getting a good loss, check the forms to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from etaprogress.progress import ProgressBar\n",
    "total = epochs\n",
    "bar = ProgressBar(total, max_width=40)\n",
    "last_epoch = 99\n",
    "show_every_n_batches = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1077"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(source_int_text) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0% (  0/100) [           ] eta --:-- /\n",
      "Epoch   0 Batch    0/1077 - Train Accuracy:  0.147, Validation Accuracy:  0.189, Loss:  5.884\n",
      "Epoch   0 Batch  300/1077 - Train Accuracy:  0.333, Validation Accuracy:  0.425, Loss:  2.845\n",
      "Epoch   0 Batch  600/1077 - Train Accuracy:  0.493, Validation Accuracy:  0.500, Loss:  2.149\n",
      "Epoch   0 Batch  900/1077 - Train Accuracy:  0.425, Validation Accuracy:  0.499, Loss:  1.836\n",
      "  1% (  1/100) [         ] eta 3:44:49 -\n",
      "Epoch   1 Batch  123/1077 - Train Accuracy:  0.433, Validation Accuracy:  0.452, Loss:  1.312\n",
      "Epoch   1 Batch  423/1077 - Train Accuracy:  0.447, Validation Accuracy:  0.479, Loss:  1.194\n",
      "Epoch   1 Batch  723/1077 - Train Accuracy:  0.513, Validation Accuracy:  0.517, Loss:  1.003\n",
      "Epoch   1 Batch 1023/1077 - Train Accuracy:  0.532, Validation Accuracy:  0.542, Loss:  0.868\n",
      "  2% (  2/100) [         ] eta 3:41:15 \\\n",
      "Epoch   2 Batch  246/1077 - Train Accuracy:  0.529, Validation Accuracy:  0.572, Loss:  0.862\n",
      "Epoch   2 Batch  546/1077 - Train Accuracy:  0.519, Validation Accuracy:  0.592, Loss:  0.867\n",
      "Epoch   2 Batch  846/1077 - Train Accuracy:  0.584, Validation Accuracy:  0.604, Loss:  0.774\n",
      "  3% (  3/100) [         ] eta 3:23:01 |\n",
      "Epoch   3 Batch   69/1077 - Train Accuracy:  0.589, Validation Accuracy:  0.604, Loss:  0.765\n",
      "Epoch   3 Batch  369/1077 - Train Accuracy:  0.578, Validation Accuracy:  0.613, Loss:  0.692\n",
      "Epoch   3 Batch  669/1077 - Train Accuracy:  0.596, Validation Accuracy:  0.608, Loss:  0.683\n",
      "Epoch   3 Batch  969/1077 - Train Accuracy:  0.631, Validation Accuracy:  0.613, Loss:  0.674\n",
      "  4% (  4/100) [         ] eta 3:19:54 /\n",
      "Epoch   4 Batch  192/1077 - Train Accuracy:  0.621, Validation Accuracy:  0.619, Loss:  0.661\n",
      "Epoch   4 Batch  492/1077 - Train Accuracy:  0.608, Validation Accuracy:  0.631, Loss:  0.650\n",
      "Epoch   4 Batch  792/1077 - Train Accuracy:  0.601, Validation Accuracy:  0.631, Loss:  0.620\n",
      "  5% (  5/100) [         ] eta 3:12:26 -\n",
      "Epoch   5 Batch   15/1077 - Train Accuracy:  0.657, Validation Accuracy:  0.651, Loss:  0.582\n",
      "Epoch   5 Batch  315/1077 - Train Accuracy:  0.653, Validation Accuracy:  0.651, Loss:  0.550\n",
      "Epoch   5 Batch  615/1077 - Train Accuracy:  0.671, Validation Accuracy:  0.667, Loss:  0.545\n",
      "Epoch   5 Batch  915/1077 - Train Accuracy:  0.619, Validation Accuracy:  0.669, Loss:  0.584\n",
      "  6% (  6/100) [         ] eta 3:10:14 \\\n",
      "Epoch   6 Batch  138/1077 - Train Accuracy:  0.652, Validation Accuracy:  0.673, Loss:  0.505\n",
      "Epoch   6 Batch  438/1077 - Train Accuracy:  0.662, Validation Accuracy:  0.692, Loss:  0.500\n",
      "Epoch   6 Batch  738/1077 - Train Accuracy:  0.720, Validation Accuracy:  0.686, Loss:  0.423\n",
      "Epoch   6 Batch 1038/1077 - Train Accuracy:  0.706, Validation Accuracy:  0.695, Loss:  0.456\n",
      "  7% (  7/100) [         ] eta 3:09:44 |\n",
      "Epoch   7 Batch  261/1077 - Train Accuracy:  0.667, Validation Accuracy:  0.702, Loss:  0.444\n",
      "Epoch   7 Batch  561/1077 - Train Accuracy:  0.714, Validation Accuracy:  0.693, Loss:  0.415\n",
      "Epoch   7 Batch  861/1077 - Train Accuracy:  0.688, Validation Accuracy:  0.702, Loss:  0.430\n",
      "  8% (  8/100) [         ] eta 3:06:27 /\n",
      "Epoch   8 Batch   84/1077 - Train Accuracy:  0.713, Validation Accuracy:  0.709, Loss:  0.440\n",
      "Epoch   8 Batch  384/1077 - Train Accuracy:  0.715, Validation Accuracy:  0.705, Loss:  0.406\n",
      "Epoch   8 Batch  684/1077 - Train Accuracy:  0.721, Validation Accuracy:  0.708, Loss:  0.395\n",
      "Epoch   8 Batch  984/1077 - Train Accuracy:  0.657, Validation Accuracy:  0.718, Loss:  0.438\n",
      "  9% (  9/100) [         ] eta 3:04:32 -\n",
      "Epoch   9 Batch  207/1077 - Train Accuracy:  0.674, Validation Accuracy:  0.712, Loss:  0.394\n",
      "Epoch   9 Batch  507/1077 - Train Accuracy:  0.687, Validation Accuracy:  0.707, Loss:  0.384\n",
      "Epoch   9 Batch  807/1077 - Train Accuracy:  0.700, Validation Accuracy:  0.716, Loss:  0.359\n",
      " 10% ( 10/100) [         ] eta 3:01:28 \\\n",
      "Epoch  10 Batch   30/1077 - Train Accuracy:  0.708, Validation Accuracy:  0.708, Loss:  0.377\n",
      "Epoch  10 Batch  330/1077 - Train Accuracy:  0.756, Validation Accuracy:  0.723, Loss:  0.386\n",
      "Epoch  10 Batch  630/1077 - Train Accuracy:  0.729, Validation Accuracy:  0.725, Loss:  0.365\n",
      "Epoch  10 Batch  930/1077 - Train Accuracy:  0.741, Validation Accuracy:  0.725, Loss:  0.359\n",
      " 11% ( 11/100) [         ] eta 2:59:16 |\n",
      "Epoch  11 Batch  153/1077 - Train Accuracy:  0.731, Validation Accuracy:  0.729, Loss:  0.390\n",
      "Epoch  11 Batch  453/1077 - Train Accuracy:  0.741, Validation Accuracy:  0.731, Loss:  0.336\n",
      "Epoch  11 Batch  753/1077 - Train Accuracy:  0.737, Validation Accuracy:  0.735, Loss:  0.342\n",
      "Epoch  11 Batch 1053/1077 - Train Accuracy:  0.747, Validation Accuracy:  0.739, Loss:  0.343\n",
      " 12% ( 12/100) [#        ] eta 2:57:46 /\n",
      "Epoch  12 Batch  276/1077 - Train Accuracy:  0.701, Validation Accuracy:  0.750, Loss:  0.375\n",
      "Epoch  12 Batch  576/1077 - Train Accuracy:  0.771, Validation Accuracy:  0.740, Loss:  0.329\n",
      "Epoch  12 Batch  876/1077 - Train Accuracy:  0.712, Validation Accuracy:  0.747, Loss:  0.343\n",
      " 13% ( 13/100) [#        ] eta 2:55:34 -\n",
      "Epoch  13 Batch   99/1077 - Train Accuracy:  0.744, Validation Accuracy:  0.742, Loss:  0.338\n",
      "Epoch  13 Batch  399/1077 - Train Accuracy:  0.729, Validation Accuracy:  0.744, Loss:  0.351\n",
      "Epoch  13 Batch  699/1077 - Train Accuracy:  0.707, Validation Accuracy:  0.749, Loss:  0.337\n",
      "Epoch  13 Batch  999/1077 - Train Accuracy:  0.768, Validation Accuracy:  0.747, Loss:  0.335\n",
      " 14% ( 14/100) [#        ] eta 2:53:33 \\\n",
      "Epoch  14 Batch  222/1077 - Train Accuracy:  0.737, Validation Accuracy:  0.741, Loss:  0.316\n",
      "Epoch  14 Batch  522/1077 - Train Accuracy:  0.725, Validation Accuracy:  0.746, Loss:  0.331\n",
      "Epoch  14 Batch  822/1077 - Train Accuracy:  0.744, Validation Accuracy:  0.750, Loss:  0.309\n",
      " 15% ( 15/100) [#        ] eta 2:51:16 |\n",
      "Epoch  15 Batch   45/1077 - Train Accuracy:  0.773, Validation Accuracy:  0.750, Loss:  0.312\n",
      "Epoch  15 Batch  345/1077 - Train Accuracy:  0.768, Validation Accuracy:  0.748, Loss:  0.297\n",
      "Epoch  15 Batch  645/1077 - Train Accuracy:  0.788, Validation Accuracy:  0.766, Loss:  0.266\n",
      "Epoch  15 Batch  945/1077 - Train Accuracy:  0.809, Validation Accuracy:  0.752, Loss:  0.286\n",
      " 16% ( 16/100) [#        ] eta 2:49:04 /\n",
      "Epoch  16 Batch  168/1077 - Train Accuracy:  0.753, Validation Accuracy:  0.771, Loss:  0.308\n",
      "Epoch  16 Batch  468/1077 - Train Accuracy:  0.787, Validation Accuracy:  0.767, Loss:  0.288\n",
      "Epoch  16 Batch  768/1077 - Train Accuracy:  0.766, Validation Accuracy:  0.768, Loss:  0.286\n",
      "Epoch  16 Batch 1068/1077 - Train Accuracy:  0.780, Validation Accuracy:  0.770, Loss:  0.268\n",
      " 17% ( 17/100) [#        ] eta 2:47:15 -\n",
      "Epoch  17 Batch  291/1077 - Train Accuracy:  0.740, Validation Accuracy:  0.766, Loss:  0.301\n",
      "Epoch  17 Batch  591/1077 - Train Accuracy:  0.800, Validation Accuracy:  0.776, Loss:  0.255\n",
      "Epoch  17 Batch  891/1077 - Train Accuracy:  0.784, Validation Accuracy:  0.777, Loss:  0.281\n",
      " 18% ( 18/100) [#        ] eta 2:45:17 \\\n",
      "Epoch  18 Batch  114/1077 - Train Accuracy:  0.781, Validation Accuracy:  0.778, Loss:  0.244\n",
      "Epoch  18 Batch  414/1077 - Train Accuracy:  0.743, Validation Accuracy:  0.782, Loss:  0.269\n",
      "Epoch  18 Batch  714/1077 - Train Accuracy:  0.757, Validation Accuracy:  0.778, Loss:  0.262\n",
      "Epoch  18 Batch 1014/1077 - Train Accuracy:  0.777, Validation Accuracy:  0.782, Loss:  0.262\n",
      " 19% ( 19/100) [#        ] eta 2:43:13 |\n",
      "Epoch  19 Batch  237/1077 - Train Accuracy:  0.772, Validation Accuracy:  0.779, Loss:  0.220\n",
      "Epoch  19 Batch  537/1077 - Train Accuracy:  0.784, Validation Accuracy:  0.775, Loss:  0.250\n",
      "Epoch  19 Batch  837/1077 - Train Accuracy:  0.776, Validation Accuracy:  0.780, Loss:  0.265\n",
      " 20% ( 20/100) [#        ] eta 2:41:10 /\n",
      "Epoch  20 Batch   60/1077 - Train Accuracy:  0.758, Validation Accuracy:  0.780, Loss:  0.240\n",
      "Epoch  20 Batch  360/1077 - Train Accuracy:  0.788, Validation Accuracy:  0.793, Loss:  0.246\n",
      "Epoch  20 Batch  660/1077 - Train Accuracy:  0.781, Validation Accuracy:  0.782, Loss:  0.256\n",
      "Epoch  20 Batch  960/1077 - Train Accuracy:  0.790, Validation Accuracy:  0.784, Loss:  0.230\n",
      " 21% ( 21/100) [#        ] eta 2:38:59 -\n",
      "Epoch  21 Batch  183/1077 - Train Accuracy:  0.788, Validation Accuracy:  0.784, Loss:  0.239\n",
      "Epoch  21 Batch  483/1077 - Train Accuracy:  0.748, Validation Accuracy:  0.789, Loss:  0.250\n",
      "Epoch  21 Batch  783/1077 - Train Accuracy:  0.770, Validation Accuracy:  0.788, Loss:  0.236\n",
      " 22% ( 22/100) [#        ] eta 2:36:55 \\\n",
      "Epoch  22 Batch    6/1077 - Train Accuracy:  0.794, Validation Accuracy:  0.786, Loss:  0.243\n",
      "Epoch  22 Batch  306/1077 - Train Accuracy:  0.791, Validation Accuracy:  0.789, Loss:  0.241\n",
      "Epoch  22 Batch  606/1077 - Train Accuracy:  0.808, Validation Accuracy:  0.789, Loss:  0.219\n",
      "Epoch  22 Batch  906/1077 - Train Accuracy:  0.779, Validation Accuracy:  0.788, Loss:  0.209\n",
      " 23% ( 23/100) [##       ] eta 2:34:41 |\n",
      "Epoch  23 Batch  129/1077 - Train Accuracy:  0.806, Validation Accuracy:  0.794, Loss:  0.218\n",
      "Epoch  23 Batch  429/1077 - Train Accuracy:  0.822, Validation Accuracy:  0.797, Loss:  0.212\n",
      "Epoch  23 Batch  729/1077 - Train Accuracy:  0.747, Validation Accuracy:  0.789, Loss:  0.241\n",
      "Epoch  23 Batch 1029/1077 - Train Accuracy:  0.772, Validation Accuracy:  0.810, Loss:  0.212\n",
      " 24% ( 24/100) [##       ] eta 2:32:39 /\n",
      "Epoch  24 Batch  252/1077 - Train Accuracy:  0.846, Validation Accuracy:  0.804, Loss:  0.200\n",
      "Epoch  24 Batch  552/1077 - Train Accuracy:  0.793, Validation Accuracy:  0.815, Loss:  0.214\n",
      "Epoch  24 Batch  852/1077 - Train Accuracy:  0.808, Validation Accuracy:  0.810, Loss:  0.222\n",
      " 25% ( 25/100) [##       ] eta 2:30:46 -\n",
      "Epoch  25 Batch   75/1077 - Train Accuracy:  0.794, Validation Accuracy:  0.820, Loss:  0.215\n",
      "Epoch  25 Batch  375/1077 - Train Accuracy:  0.824, Validation Accuracy:  0.813, Loss:  0.181\n",
      "Epoch  25 Batch  675/1077 - Train Accuracy:  0.827, Validation Accuracy:  0.821, Loss:  0.197\n",
      "Epoch  25 Batch  975/1077 - Train Accuracy:  0.833, Validation Accuracy:  0.816, Loss:  0.184\n",
      " 26% ( 26/100) [##       ] eta 2:28:38 \\\n",
      "Epoch  26 Batch  197/1077 - Train Accuracy:  0.800, Validation Accuracy:  0.811, Loss:  0.183\n",
      "Epoch  26 Batch  497/1077 - Train Accuracy:  0.769, Validation Accuracy:  0.804, Loss:  0.206\n",
      "Epoch  26 Batch  797/1077 - Train Accuracy:  0.811, Validation Accuracy:  0.824, Loss:  0.189\n",
      " 27% ( 27/100) [##       ] eta 2:26:41 |\n",
      "Epoch  27 Batch   20/1077 - Train Accuracy:  0.791, Validation Accuracy:  0.818, Loss:  0.192\n",
      "Epoch  27 Batch  320/1077 - Train Accuracy:  0.833, Validation Accuracy:  0.824, Loss:  0.188\n",
      "Epoch  27 Batch  620/1077 - Train Accuracy:  0.804, Validation Accuracy:  0.821, Loss:  0.169\n",
      "Epoch  27 Batch  920/1077 - Train Accuracy:  0.810, Validation Accuracy:  0.822, Loss:  0.197\n",
      " 28% ( 28/100) [##       ] eta 2:24:30 /\n",
      "Epoch  28 Batch  143/1077 - Train Accuracy:  0.807, Validation Accuracy:  0.824, Loss:  0.182\n",
      "Epoch  28 Batch  443/1077 - Train Accuracy:  0.843, Validation Accuracy:  0.828, Loss:  0.185\n",
      "Epoch  28 Batch  743/1077 - Train Accuracy:  0.837, Validation Accuracy:  0.819, Loss:  0.187\n",
      "Epoch  28 Batch 1043/1077 - Train Accuracy:  0.830, Validation Accuracy:  0.825, Loss:  0.211\n",
      " 28% ( 29/100) [##       ] eta 2:22:27 -\n",
      "Epoch  29 Batch  266/1077 - Train Accuracy:  0.802, Validation Accuracy:  0.820, Loss:  0.184\n",
      "Epoch  29 Batch  566/1077 - Train Accuracy:  0.821, Validation Accuracy:  0.810, Loss:  0.205\n",
      "Epoch  29 Batch  866/1077 - Train Accuracy:  0.818, Validation Accuracy:  0.830, Loss:  0.186\n",
      " 30% ( 30/100) [##       ] eta 2:20:36 \\\n",
      "Epoch  30 Batch   89/1077 - Train Accuracy:  0.802, Validation Accuracy:  0.831, Loss:  0.191\n",
      "Epoch  30 Batch  389/1077 - Train Accuracy:  0.846, Validation Accuracy:  0.833, Loss:  0.167\n",
      "Epoch  30 Batch  689/1077 - Train Accuracy:  0.834, Validation Accuracy:  0.839, Loss:  0.159\n",
      "Epoch  30 Batch  989/1077 - Train Accuracy:  0.839, Validation Accuracy:  0.849, Loss:  0.177\n",
      " 31% ( 31/100) [##       ] eta 2:18:29 |\n",
      "Epoch  31 Batch  212/1077 - Train Accuracy:  0.858, Validation Accuracy:  0.844, Loss:  0.145\n",
      "Epoch  31 Batch  512/1077 - Train Accuracy:  0.883, Validation Accuracy:  0.855, Loss:  0.151\n",
      "Epoch  31 Batch  812/1077 - Train Accuracy:  0.857, Validation Accuracy:  0.858, Loss:  0.155\n",
      " 32% ( 32/100) [##       ] eta 2:16:36 /\n",
      "Epoch  32 Batch   35/1077 - Train Accuracy:  0.902, Validation Accuracy:  0.858, Loss:  0.156\n",
      "Epoch  32 Batch  335/1077 - Train Accuracy:  0.880, Validation Accuracy:  0.858, Loss:  0.158\n",
      "Epoch  32 Batch  635/1077 - Train Accuracy:  0.888, Validation Accuracy:  0.851, Loss:  0.179\n",
      "Epoch  32 Batch  935/1077 - Train Accuracy:  0.898, Validation Accuracy:  0.869, Loss:  0.148\n",
      " 33% ( 33/100) [##       ] eta 2:14:26 -\n",
      "Epoch  33 Batch  158/1077 - Train Accuracy:  0.897, Validation Accuracy:  0.855, Loss:  0.150\n",
      "Epoch  33 Batch  458/1077 - Train Accuracy:  0.882, Validation Accuracy:  0.879, Loss:  0.155\n",
      "Epoch  33 Batch  758/1077 - Train Accuracy:  0.873, Validation Accuracy:  0.879, Loss:  0.144\n",
      "Epoch  33 Batch 1058/1077 - Train Accuracy:  0.906, Validation Accuracy:  0.882, Loss:  0.153\n",
      " 34% ( 34/100) [###      ] eta 2:12:22 \\\n",
      "Epoch  34 Batch  281/1077 - Train Accuracy:  0.902, Validation Accuracy:  0.889, Loss:  0.160\n",
      "Epoch  34 Batch  581/1077 - Train Accuracy:  0.929, Validation Accuracy:  0.881, Loss:  0.120\n",
      "Epoch  34 Batch  881/1077 - Train Accuracy:  0.911, Validation Accuracy:  0.874, Loss:  0.137\n",
      " 35% ( 35/100) [###      ] eta 2:10:33 |\n",
      "Epoch  35 Batch  104/1077 - Train Accuracy:  0.877, Validation Accuracy:  0.880, Loss:  0.150\n",
      "Epoch  35 Batch  404/1077 - Train Accuracy:  0.937, Validation Accuracy:  0.869, Loss:  0.130\n",
      "Epoch  35 Batch  704/1077 - Train Accuracy:  0.877, Validation Accuracy:  0.895, Loss:  0.155\n",
      "Epoch  35 Batch 1004/1077 - Train Accuracy:  0.922, Validation Accuracy:  0.877, Loss:  0.135\n",
      " 36% ( 36/100) [###      ] eta 2:08:26 /\n",
      "Epoch  36 Batch  227/1077 - Train Accuracy:  0.872, Validation Accuracy:  0.896, Loss:  0.149\n",
      "Epoch  36 Batch  527/1077 - Train Accuracy:  0.888, Validation Accuracy:  0.884, Loss:  0.111\n",
      "Epoch  36 Batch  827/1077 - Train Accuracy:  0.912, Validation Accuracy:  0.893, Loss:  0.146\n",
      " 37% ( 37/100) [###      ] eta 2:06:33 -\n",
      "Epoch  37 Batch   50/1077 - Train Accuracy:  0.921, Validation Accuracy:  0.886, Loss:  0.123\n",
      "Epoch  37 Batch  350/1077 - Train Accuracy:  0.901, Validation Accuracy:  0.890, Loss:  0.115\n",
      "Epoch  37 Batch  650/1077 - Train Accuracy:  0.920, Validation Accuracy:  0.890, Loss:  0.120\n",
      "Epoch  37 Batch  950/1077 - Train Accuracy:  0.917, Validation Accuracy:  0.880, Loss:  0.106\n",
      " 38% ( 38/100) [###      ] eta 2:04:25 \\\n",
      "Epoch  38 Batch  173/1077 - Train Accuracy:  0.907, Validation Accuracy:  0.888, Loss:  0.134\n",
      "Epoch  38 Batch  473/1077 - Train Accuracy:  0.918, Validation Accuracy:  0.893, Loss:  0.123\n",
      "Epoch  38 Batch  773/1077 - Train Accuracy:  0.917, Validation Accuracy:  0.886, Loss:  0.114\n",
      "Epoch  38 Batch 1073/1077 - Train Accuracy:  0.914, Validation Accuracy:  0.898, Loss:  0.143\n",
      " 39% ( 39/100) [###      ] eta 2:02:20 |\n",
      "Epoch  39 Batch  296/1077 - Train Accuracy:  0.940, Validation Accuracy:  0.886, Loss:  0.114\n",
      "Epoch  39 Batch  596/1077 - Train Accuracy:  0.926, Validation Accuracy:  0.888, Loss:  0.120\n",
      "Epoch  39 Batch  896/1077 - Train Accuracy:  0.891, Validation Accuracy:  0.890, Loss:  0.125\n",
      " 40% ( 40/100) [###      ] eta 2:00:30 /\n",
      "Epoch  40 Batch  119/1077 - Train Accuracy:  0.896, Validation Accuracy:  0.886, Loss:  0.129\n",
      "Epoch  40 Batch  419/1077 - Train Accuracy:  0.913, Validation Accuracy:  0.896, Loss:  0.110\n",
      "Epoch  40 Batch  719/1077 - Train Accuracy:  0.924, Validation Accuracy:  0.897, Loss:  0.122\n",
      "Epoch  40 Batch 1019/1077 - Train Accuracy:  0.875, Validation Accuracy:  0.888, Loss:  0.135\n",
      " 41% ( 41/100) [###      ] eta 1:58:23 -\n",
      "Epoch  41 Batch  242/1077 - Train Accuracy:  0.945, Validation Accuracy:  0.904, Loss:  0.109\n",
      "Epoch  41 Batch  542/1077 - Train Accuracy:  0.914, Validation Accuracy:  0.902, Loss:  0.115\n",
      "Epoch  41 Batch  842/1077 - Train Accuracy:  0.942, Validation Accuracy:  0.890, Loss:  0.103\n",
      " 42% ( 42/100) [###      ] eta 1:56:31 \\\n",
      "Epoch  42 Batch   65/1077 - Train Accuracy:  0.929, Validation Accuracy:  0.898, Loss:  0.111\n",
      "Epoch  42 Batch  365/1077 - Train Accuracy:  0.938, Validation Accuracy:  0.895, Loss:  0.109\n",
      "Epoch  42 Batch  665/1077 - Train Accuracy:  0.906, Validation Accuracy:  0.888, Loss:  0.122\n",
      "Epoch  42 Batch  965/1077 - Train Accuracy:  0.910, Validation Accuracy:  0.904, Loss:  0.141\n",
      " 43% ( 43/100) [###      ] eta 1:54:24 |\n",
      "Epoch  43 Batch  188/1077 - Train Accuracy:  0.921, Validation Accuracy:  0.902, Loss:  0.107\n",
      "Epoch  43 Batch  488/1077 - Train Accuracy:  0.934, Validation Accuracy:  0.898, Loss:  0.112\n",
      "Epoch  43 Batch  788/1077 - Train Accuracy:  0.923, Validation Accuracy:  0.892, Loss:  0.108\n",
      " 44% ( 44/100) [###      ] eta 1:52:30 /\n",
      "Epoch  44 Batch   11/1077 - Train Accuracy:  0.914, Validation Accuracy:  0.896, Loss:  0.118\n",
      "Epoch  44 Batch  311/1077 - Train Accuracy:  0.919, Validation Accuracy:  0.908, Loss:  0.099\n",
      "Epoch  44 Batch  611/1077 - Train Accuracy:  0.910, Validation Accuracy:  0.892, Loss:  0.104\n",
      "Epoch  44 Batch  911/1077 - Train Accuracy:  0.928, Validation Accuracy:  0.912, Loss:  0.103\n",
      " 45% ( 45/100) [####     ] eta 1:50:22 -\n",
      "Epoch  45 Batch  134/1077 - Train Accuracy:  0.960, Validation Accuracy:  0.910, Loss:  0.085\n",
      "Epoch  45 Batch  434/1077 - Train Accuracy:  0.931, Validation Accuracy:  0.909, Loss:  0.097\n",
      "Epoch  45 Batch  734/1077 - Train Accuracy:  0.935, Validation Accuracy:  0.907, Loss:  0.105\n",
      "Epoch  45 Batch 1034/1077 - Train Accuracy:  0.910, Validation Accuracy:  0.897, Loss:  0.104\n",
      " 46% ( 46/100) [####     ] eta 1:48:16 \\\n",
      "Epoch  46 Batch  257/1077 - Train Accuracy:  0.927, Validation Accuracy:  0.903, Loss:  0.094\n",
      "Epoch  46 Batch  557/1077 - Train Accuracy:  0.929, Validation Accuracy:  0.911, Loss:  0.092\n",
      "Epoch  46 Batch  857/1077 - Train Accuracy:  0.922, Validation Accuracy:  0.910, Loss:  0.091\n",
      " 47% ( 47/100) [####     ] eta 1:46:24 |\n",
      "Epoch  47 Batch   80/1077 - Train Accuracy:  0.936, Validation Accuracy:  0.925, Loss:  0.089\n",
      "Epoch  47 Batch  380/1077 - Train Accuracy:  0.941, Validation Accuracy:  0.905, Loss:  0.094\n",
      "Epoch  47 Batch  680/1077 - Train Accuracy:  0.933, Validation Accuracy:  0.910, Loss:  0.099\n",
      "Epoch  47 Batch  980/1077 - Train Accuracy:  0.886, Validation Accuracy:  0.915, Loss:  0.099\n",
      " 48% ( 48/100) [####     ] eta 1:44:17 /\n",
      "Epoch  48 Batch  203/1077 - Train Accuracy:  0.917, Validation Accuracy:  0.907, Loss:  0.090\n",
      "Epoch  48 Batch  503/1077 - Train Accuracy:  0.941, Validation Accuracy:  0.924, Loss:  0.082\n",
      "Epoch  48 Batch  803/1077 - Train Accuracy:  0.934, Validation Accuracy:  0.910, Loss:  0.091\n",
      " 49% ( 49/100) [####     ] eta 1:42:25 -\n",
      "Epoch  49 Batch   26/1077 - Train Accuracy:  0.915, Validation Accuracy:  0.915, Loss:  0.103\n",
      "Epoch  49 Batch  326/1077 - Train Accuracy:  0.951, Validation Accuracy:  0.901, Loss:  0.081\n",
      "Epoch  49 Batch  626/1077 - Train Accuracy:  0.939, Validation Accuracy:  0.918, Loss:  0.078\n",
      "Epoch  49 Batch  926/1077 - Train Accuracy:  0.920, Validation Accuracy:  0.928, Loss:  0.090\n",
      " 50% ( 50/100) [####     ] eta 1:40:17 \\\n",
      "Epoch  50 Batch  149/1077 - Train Accuracy:  0.934, Validation Accuracy:  0.922, Loss:  0.085\n",
      "Epoch  50 Batch  449/1077 - Train Accuracy:  0.934, Validation Accuracy:  0.917, Loss:  0.096\n",
      "Epoch  50 Batch  749/1077 - Train Accuracy:  0.942, Validation Accuracy:  0.922, Loss:  0.081\n",
      "Epoch  50 Batch 1049/1077 - Train Accuracy:  0.925, Validation Accuracy:  0.922, Loss:  0.095\n",
      " 51% ( 51/100) [####     ] eta 1:38:12 |\n",
      "Epoch  51 Batch  272/1077 - Train Accuracy:  0.954, Validation Accuracy:  0.920, Loss:  0.121\n",
      "Epoch  51 Batch  572/1077 - Train Accuracy:  0.928, Validation Accuracy:  0.903, Loss:  0.069\n",
      "Epoch  51 Batch  872/1077 - Train Accuracy:  0.959, Validation Accuracy:  0.914, Loss:  0.085\n",
      " 52% ( 52/100) [####     ] eta 1:36:20 /\n",
      "Epoch  52 Batch   94/1077 - Train Accuracy:  0.966, Validation Accuracy:  0.918, Loss:  0.054\n",
      "Epoch  52 Batch  394/1077 - Train Accuracy:  0.931, Validation Accuracy:  0.917, Loss:  0.076\n",
      "Epoch  52 Batch  694/1077 - Train Accuracy:  0.952, Validation Accuracy:  0.920, Loss:  0.079\n",
      "Epoch  52 Batch  994/1077 - Train Accuracy:  0.948, Validation Accuracy:  0.915, Loss:  0.077\n",
      " 53% ( 53/100) [####     ] eta 1:34:14 -\n",
      "Epoch  53 Batch  217/1077 - Train Accuracy:  0.955, Validation Accuracy:  0.916, Loss:  0.068\n",
      "Epoch  53 Batch  517/1077 - Train Accuracy:  0.935, Validation Accuracy:  0.925, Loss:  0.070\n",
      "Epoch  53 Batch  817/1077 - Train Accuracy:  0.934, Validation Accuracy:  0.925, Loss:  0.078\n",
      " 54% ( 54/100) [####     ] eta 1:32:21 \\\n",
      "Epoch  54 Batch   40/1077 - Train Accuracy:  0.968, Validation Accuracy:  0.918, Loss:  0.074\n",
      "Epoch  54 Batch  340/1077 - Train Accuracy:  0.935, Validation Accuracy:  0.936, Loss:  0.077\n",
      "Epoch  54 Batch  640/1077 - Train Accuracy:  0.917, Validation Accuracy:  0.930, Loss:  0.063\n",
      "Epoch  54 Batch  940/1077 - Train Accuracy:  0.924, Validation Accuracy:  0.910, Loss:  0.066\n",
      " 55% ( 55/100) [####     ] eta 1:30:14 |\n",
      "Epoch  55 Batch  163/1077 - Train Accuracy:  0.943, Validation Accuracy:  0.926, Loss:  0.097\n",
      "Epoch  55 Batch  463/1077 - Train Accuracy:  0.921, Validation Accuracy:  0.901, Loss:  0.082\n",
      "Epoch  55 Batch  763/1077 - Train Accuracy:  0.941, Validation Accuracy:  0.916, Loss:  0.076\n",
      "Epoch  55 Batch 1063/1077 - Train Accuracy:  0.921, Validation Accuracy:  0.920, Loss:  0.087\n",
      " 56% ( 56/100) [#####    ] eta 1:28:10 /\n",
      "Epoch  56 Batch  286/1077 - Train Accuracy:  0.948, Validation Accuracy:  0.910, Loss:  0.068\n",
      "Epoch  56 Batch  586/1077 - Train Accuracy:  0.942, Validation Accuracy:  0.928, Loss:  0.070\n",
      "Epoch  56 Batch  886/1077 - Train Accuracy:  0.951, Validation Accuracy:  0.926, Loss:  0.079\n",
      " 56% ( 57/100) [#####    ] eta 1:26:17 -\n",
      "Epoch  57 Batch  109/1077 - Train Accuracy:  0.936, Validation Accuracy:  0.914, Loss:  0.086\n",
      "Epoch  57 Batch  409/1077 - Train Accuracy:  0.940, Validation Accuracy:  0.927, Loss:  0.075\n",
      "Epoch  57 Batch  709/1077 - Train Accuracy:  0.925, Validation Accuracy:  0.925, Loss:  0.085\n",
      "Epoch  57 Batch 1009/1077 - Train Accuracy:  0.984, Validation Accuracy:  0.933, Loss:  0.053\n",
      " 57% ( 58/100) [#####    ] eta 1:24:12 \\\n",
      "Epoch  58 Batch  232/1077 - Train Accuracy:  0.965, Validation Accuracy:  0.928, Loss:  0.055\n",
      "Epoch  58 Batch  532/1077 - Train Accuracy:  0.911, Validation Accuracy:  0.934, Loss:  0.078\n",
      "Epoch  58 Batch  832/1077 - Train Accuracy:  0.955, Validation Accuracy:  0.929, Loss:  0.070\n",
      " 59% ( 59/100) [#####    ] eta 1:22:19 |\n",
      "Epoch  59 Batch   55/1077 - Train Accuracy:  0.955, Validation Accuracy:  0.924, Loss:  0.065\n",
      "Epoch  59 Batch  355/1077 - Train Accuracy:  0.947, Validation Accuracy:  0.930, Loss:  0.072\n",
      "Epoch  59 Batch  655/1077 - Train Accuracy:  0.945, Validation Accuracy:  0.930, Loss:  0.071\n",
      "Epoch  59 Batch  955/1077 - Train Accuracy:  0.952, Validation Accuracy:  0.929, Loss:  0.080\n",
      " 60% ( 60/100) [#####    ] eta 1:20:11 /\n",
      "Epoch  60 Batch  178/1077 - Train Accuracy:  0.948, Validation Accuracy:  0.925, Loss:  0.054\n",
      "Epoch  60 Batch  478/1077 - Train Accuracy:  0.952, Validation Accuracy:  0.934, Loss:  0.066\n",
      "Epoch  60 Batch  778/1077 - Train Accuracy:  0.945, Validation Accuracy:  0.950, Loss:  0.065\n",
      " 61% ( 61/100) [#####    ] eta 1:18:17 -\n",
      "Epoch  61 Batch    1/1077 - Train Accuracy:  0.963, Validation Accuracy:  0.933, Loss:  0.049\n",
      "Epoch  61 Batch  301/1077 - Train Accuracy:  0.961, Validation Accuracy:  0.939, Loss:  0.056\n",
      "Epoch  61 Batch  601/1077 - Train Accuracy:  0.931, Validation Accuracy:  0.938, Loss:  0.064\n",
      "Epoch  61 Batch  901/1077 - Train Accuracy:  0.943, Validation Accuracy:  0.938, Loss:  0.073\n",
      " 62% ( 62/100) [#####    ] eta 1:16:12 \\\n",
      "Epoch  62 Batch  124/1077 - Train Accuracy:  0.937, Validation Accuracy:  0.930, Loss:  0.072\n",
      "Epoch  62 Batch  424/1077 - Train Accuracy:  0.943, Validation Accuracy:  0.941, Loss:  0.071\n",
      "Epoch  62 Batch  724/1077 - Train Accuracy:  0.925, Validation Accuracy:  0.931, Loss:  0.081\n",
      "Epoch  62 Batch 1024/1077 - Train Accuracy:  0.922, Validation Accuracy:  0.931, Loss:  0.071\n",
      " 63% ( 63/100) [#####    ] eta 1:14:07 |\n",
      "Epoch  63 Batch  247/1077 - Train Accuracy:  0.943, Validation Accuracy:  0.942, Loss:  0.060\n",
      "Epoch  63 Batch  547/1077 - Train Accuracy:  0.943, Validation Accuracy:  0.939, Loss:  0.058\n",
      "Epoch  63 Batch  847/1077 - Train Accuracy:  0.939, Validation Accuracy:  0.942, Loss:  0.078\n",
      " 64% ( 64/100) [#####    ] eta 1:12:13 /\n",
      "Epoch  64 Batch   70/1077 - Train Accuracy:  0.922, Validation Accuracy:  0.922, Loss:  0.062\n",
      "Epoch  64 Batch  370/1077 - Train Accuracy:  0.958, Validation Accuracy:  0.928, Loss:  0.055\n",
      "Epoch  64 Batch  670/1077 - Train Accuracy:  0.960, Validation Accuracy:  0.939, Loss:  0.061\n",
      "Epoch  64 Batch  970/1077 - Train Accuracy:  0.952, Validation Accuracy:  0.946, Loss:  0.063\n",
      " 65% ( 65/100) [#####    ] eta 1:10:08 -\n",
      "Epoch  65 Batch  193/1077 - Train Accuracy:  0.952, Validation Accuracy:  0.938, Loss:  0.052\n",
      "Epoch  65 Batch  493/1077 - Train Accuracy:  0.966, Validation Accuracy:  0.930, Loss:  0.046\n",
      "Epoch  65 Batch  793/1077 - Train Accuracy:  0.978, Validation Accuracy:  0.945, Loss:  0.056\n",
      " 66% ( 66/100) [#####    ] eta 1:08:13 \\\n",
      "Epoch  66 Batch   16/1077 - Train Accuracy:  0.958, Validation Accuracy:  0.951, Loss:  0.069\n",
      "Epoch  66 Batch  316/1077 - Train Accuracy:  0.967, Validation Accuracy:  0.955, Loss:  0.054\n",
      "Epoch  66 Batch  616/1077 - Train Accuracy:  0.954, Validation Accuracy:  0.926, Loss:  0.056\n",
      "Epoch  66 Batch  916/1077 - Train Accuracy:  0.966, Validation Accuracy:  0.952, Loss:  0.057\n",
      " 67% ( 67/100) [######   ] eta 1:06:09 |\n",
      "Epoch  67 Batch  139/1077 - Train Accuracy:  0.942, Validation Accuracy:  0.947, Loss:  0.061\n",
      "Epoch  67 Batch  439/1077 - Train Accuracy:  0.941, Validation Accuracy:  0.942, Loss:  0.063\n",
      "Epoch  67 Batch  739/1077 - Train Accuracy:  0.963, Validation Accuracy:  0.937, Loss:  0.056\n",
      "Epoch  67 Batch 1039/1077 - Train Accuracy:  0.952, Validation Accuracy:  0.936, Loss:  0.057\n",
      " 68% ( 68/100) [######   ] eta 1:04:05 /\n",
      "Epoch  68 Batch  262/1077 - Train Accuracy:  0.943, Validation Accuracy:  0.941, Loss:  0.053\n",
      "Epoch  68 Batch  562/1077 - Train Accuracy:  0.947, Validation Accuracy:  0.936, Loss:  0.054\n",
      "Epoch  68 Batch  862/1077 - Train Accuracy:  0.933, Validation Accuracy:  0.937, Loss:  0.063\n",
      " 69% ( 69/100) [######   ] eta 1:02:11 -\n",
      "Epoch  69 Batch   85/1077 - Train Accuracy:  0.958, Validation Accuracy:  0.945, Loss:  0.050\n",
      "Epoch  69 Batch  385/1077 - Train Accuracy:  0.957, Validation Accuracy:  0.939, Loss:  0.055\n",
      "Epoch  69 Batch  685/1077 - Train Accuracy:  0.929, Validation Accuracy:  0.954, Loss:  0.059\n",
      "Epoch  69 Batch  985/1077 - Train Accuracy:  0.975, Validation Accuracy:  0.951, Loss:  0.037\n",
      " 70% ( 70/100) [######   ] eta 1:00:06 \\\n",
      "Epoch  70 Batch  208/1077 - Train Accuracy:  0.941, Validation Accuracy:  0.940, Loss:  0.058\n",
      "Epoch  70 Batch  508/1077 - Train Accuracy:  0.955, Validation Accuracy:  0.947, Loss:  0.053\n",
      "Epoch  70 Batch  808/1077 - Train Accuracy:  0.944, Validation Accuracy:  0.938, Loss:  0.061\n",
      " 71% ( 71/100) [#######    ] eta 58:11 |\n",
      "Epoch  71 Batch   31/1077 - Train Accuracy:  0.968, Validation Accuracy:  0.945, Loss:  0.052\n",
      "Epoch  71 Batch  331/1077 - Train Accuracy:  0.968, Validation Accuracy:  0.953, Loss:  0.051\n",
      "Epoch  71 Batch  631/1077 - Train Accuracy:  0.963, Validation Accuracy:  0.959, Loss:  0.050\n",
      "Epoch  71 Batch  931/1077 - Train Accuracy:  0.956, Validation Accuracy:  0.944, Loss:  0.054\n",
      " 72% ( 72/100) [#######    ] eta 56:08 /\n",
      "Epoch  72 Batch  154/1077 - Train Accuracy:  0.954, Validation Accuracy:  0.949, Loss:  0.054\n",
      "Epoch  72 Batch  454/1077 - Train Accuracy:  0.941, Validation Accuracy:  0.951, Loss:  0.056\n",
      "Epoch  72 Batch  754/1077 - Train Accuracy:  0.941, Validation Accuracy:  0.962, Loss:  0.059\n",
      "Epoch  72 Batch 1054/1077 - Train Accuracy:  0.948, Validation Accuracy:  0.951, Loss:  0.044\n",
      " 73% ( 73/100) [########   ] eta 54:04 -\n",
      "Epoch  73 Batch  277/1077 - Train Accuracy:  0.964, Validation Accuracy:  0.947, Loss:  0.047\n",
      "Epoch  73 Batch  577/1077 - Train Accuracy:  0.969, Validation Accuracy:  0.941, Loss:  0.059\n",
      "Epoch  73 Batch  877/1077 - Train Accuracy:  0.958, Validation Accuracy:  0.951, Loss:  0.043\n",
      " 74% ( 74/100) [########   ] eta 52:10 \\\n",
      "Epoch  74 Batch  100/1077 - Train Accuracy:  0.950, Validation Accuracy:  0.956, Loss:  0.051\n",
      "Epoch  74 Batch  400/1077 - Train Accuracy:  0.945, Validation Accuracy:  0.962, Loss:  0.060\n",
      "Epoch  74 Batch  700/1077 - Train Accuracy:  0.966, Validation Accuracy:  0.947, Loss:  0.051\n",
      "Epoch  74 Batch 1000/1077 - Train Accuracy:  0.943, Validation Accuracy:  0.941, Loss:  0.052\n",
      " 75% ( 75/100) [########   ] eta 50:06 |\n",
      "Epoch  75 Batch  223/1077 - Train Accuracy:  0.962, Validation Accuracy:  0.962, Loss:  0.043\n",
      "Epoch  75 Batch  523/1077 - Train Accuracy:  0.949, Validation Accuracy:  0.951, Loss:  0.052\n",
      "Epoch  75 Batch  823/1077 - Train Accuracy:  0.963, Validation Accuracy:  0.955, Loss:  0.048\n",
      " 76% ( 76/100) [########   ] eta 48:10 /\n",
      "Epoch  76 Batch   46/1077 - Train Accuracy:  0.956, Validation Accuracy:  0.963, Loss:  0.051\n",
      "Epoch  76 Batch  346/1077 - Train Accuracy:  0.959, Validation Accuracy:  0.949, Loss:  0.051\n",
      "Epoch  76 Batch  646/1077 - Train Accuracy:  0.966, Validation Accuracy:  0.945, Loss:  0.048\n",
      "Epoch  76 Batch  946/1077 - Train Accuracy:  0.990, Validation Accuracy:  0.954, Loss:  0.037\n",
      " 77% ( 77/100) [########   ] eta 46:08 -\n",
      "Epoch  77 Batch  168/1077 - Train Accuracy:  0.945, Validation Accuracy:  0.956, Loss:  0.064\n",
      "Epoch  77 Batch  468/1077 - Train Accuracy:  0.960, Validation Accuracy:  0.944, Loss:  0.048\n",
      "Epoch  77 Batch  768/1077 - Train Accuracy:  0.962, Validation Accuracy:  0.963, Loss:  0.049\n",
      "Epoch  77 Batch 1068/1077 - Train Accuracy:  0.970, Validation Accuracy:  0.951, Loss:  0.032\n",
      " 78% ( 78/100) [########   ] eta 44:05 \\\n",
      "Epoch  78 Batch  291/1077 - Train Accuracy:  0.929, Validation Accuracy:  0.937, Loss:  0.073\n",
      "Epoch  78 Batch  591/1077 - Train Accuracy:  0.946, Validation Accuracy:  0.947, Loss:  0.047\n",
      "Epoch  78 Batch  891/1077 - Train Accuracy:  0.968, Validation Accuracy:  0.960, Loss:  0.051\n",
      " 79% ( 79/100) [########   ] eta 42:09 |\n",
      "Epoch  79 Batch  114/1077 - Train Accuracy:  0.968, Validation Accuracy:  0.958, Loss:  0.039\n",
      "Epoch  79 Batch  414/1077 - Train Accuracy:  0.944, Validation Accuracy:  0.946, Loss:  0.057\n",
      "Epoch  79 Batch  714/1077 - Train Accuracy:  0.964, Validation Accuracy:  0.946, Loss:  0.047\n",
      "Epoch  79 Batch 1014/1077 - Train Accuracy:  0.956, Validation Accuracy:  0.958, Loss:  0.046\n",
      " 80% ( 80/100) [########   ] eta 40:06 /\n",
      "Epoch  80 Batch  237/1077 - Train Accuracy:  0.974, Validation Accuracy:  0.960, Loss:  0.034\n",
      "Epoch  80 Batch  537/1077 - Train Accuracy:  0.970, Validation Accuracy:  0.956, Loss:  0.033\n",
      "Epoch  80 Batch  837/1077 - Train Accuracy:  0.952, Validation Accuracy:  0.960, Loss:  0.051\n",
      " 81% ( 81/100) [########   ] eta 38:10 -\n",
      "Epoch  81 Batch   60/1077 - Train Accuracy:  0.980, Validation Accuracy:  0.955, Loss:  0.038\n",
      "Epoch  81 Batch  360/1077 - Train Accuracy:  0.984, Validation Accuracy:  0.951, Loss:  0.043\n",
      "Epoch  81 Batch  660/1077 - Train Accuracy:  0.984, Validation Accuracy:  0.951, Loss:  0.041\n",
      "Epoch  81 Batch  960/1077 - Train Accuracy:  0.959, Validation Accuracy:  0.958, Loss:  0.043\n",
      " 82% ( 82/100) [#########  ] eta 36:06 \\\n",
      "Epoch  82 Batch  183/1077 - Train Accuracy:  0.955, Validation Accuracy:  0.963, Loss:  0.058\n",
      "Epoch  82 Batch  483/1077 - Train Accuracy:  0.955, Validation Accuracy:  0.958, Loss:  0.049\n",
      "Epoch  82 Batch  783/1077 - Train Accuracy:  0.959, Validation Accuracy:  0.969, Loss:  0.061\n",
      " 83% ( 83/100) [#########  ] eta 34:09 |\n",
      "Epoch  83 Batch    6/1077 - Train Accuracy:  0.980, Validation Accuracy:  0.961, Loss:  0.045\n",
      "Epoch  83 Batch  306/1077 - Train Accuracy:  0.958, Validation Accuracy:  0.963, Loss:  0.043\n",
      "Epoch  83 Batch  606/1077 - Train Accuracy:  0.967, Validation Accuracy:  0.949, Loss:  0.032\n",
      "Epoch  83 Batch  906/1077 - Train Accuracy:  0.946, Validation Accuracy:  0.962, Loss:  0.056\n",
      " 84% ( 84/100) [#########  ] eta 32:07 /\n",
      "Epoch  84 Batch  129/1077 - Train Accuracy:  0.951, Validation Accuracy:  0.959, Loss:  0.050\n",
      "Epoch  84 Batch  429/1077 - Train Accuracy:  0.982, Validation Accuracy:  0.958, Loss:  0.031\n",
      "Epoch  84 Batch  729/1077 - Train Accuracy:  0.935, Validation Accuracy:  0.962, Loss:  0.065\n",
      "Epoch  84 Batch 1029/1077 - Train Accuracy:  0.957, Validation Accuracy:  0.953, Loss:  0.052\n",
      " 85% ( 85/100) [#########  ] eta 30:04 -\n",
      "Epoch  85 Batch  252/1077 - Train Accuracy:  0.964, Validation Accuracy:  0.950, Loss:  0.056\n",
      "Epoch  85 Batch  552/1077 - Train Accuracy:  0.959, Validation Accuracy:  0.953, Loss:  0.048\n",
      "Epoch  85 Batch  852/1077 - Train Accuracy:  0.944, Validation Accuracy:  0.958, Loss:  0.057\n",
      " 86% ( 86/100) [#########  ] eta 28:07 \\\n",
      "Epoch  86 Batch   75/1077 - Train Accuracy:  0.945, Validation Accuracy:  0.961, Loss:  0.072\n",
      "Epoch  86 Batch  375/1077 - Train Accuracy:  0.973, Validation Accuracy:  0.957, Loss:  0.035\n",
      "Epoch  86 Batch  675/1077 - Train Accuracy:  0.958, Validation Accuracy:  0.963, Loss:  0.045\n",
      "Epoch  86 Batch  975/1077 - Train Accuracy:  0.974, Validation Accuracy:  0.957, Loss:  0.045\n",
      " 87% ( 87/100) [#########  ] eta 26:05 |\n",
      "Epoch  87 Batch  198/1077 - Train Accuracy:  0.956, Validation Accuracy:  0.959, Loss:  0.048\n",
      "Epoch  87 Batch  498/1077 - Train Accuracy:  0.955, Validation Accuracy:  0.951, Loss:  0.044\n",
      "Epoch  87 Batch  798/1077 - Train Accuracy:  0.969, Validation Accuracy:  0.954, Loss:  0.040\n",
      " 88% ( 88/100) [#########  ] eta 24:06 /\n",
      "Epoch  88 Batch   21/1077 - Train Accuracy:  0.966, Validation Accuracy:  0.966, Loss:  0.049\n",
      "Epoch  88 Batch  321/1077 - Train Accuracy:  0.960, Validation Accuracy:  0.968, Loss:  0.040\n",
      "Epoch  88 Batch  621/1077 - Train Accuracy:  0.985, Validation Accuracy:  0.958, Loss:  0.049\n",
      "Epoch  88 Batch  921/1077 - Train Accuracy:  0.952, Validation Accuracy:  0.955, Loss:  0.043\n",
      " 89% ( 89/100) [#########  ] eta 22:05 -\n",
      "Epoch  89 Batch  144/1077 - Train Accuracy:  0.974, Validation Accuracy:  0.955, Loss:  0.053\n",
      "Epoch  89 Batch  444/1077 - Train Accuracy:  0.967, Validation Accuracy:  0.953, Loss:  0.041\n",
      "Epoch  89 Batch  744/1077 - Train Accuracy:  0.984, Validation Accuracy:  0.957, Loss:  0.048\n",
      "Epoch  89 Batch 1044/1077 - Train Accuracy:  0.969, Validation Accuracy:  0.954, Loss:  0.058\n",
      " 90% ( 90/100) [#########  ] eta 20:03 \\\n",
      "Epoch  90 Batch  267/1077 - Train Accuracy:  0.961, Validation Accuracy:  0.969, Loss:  0.041\n",
      "Epoch  90 Batch  567/1077 - Train Accuracy:  0.978, Validation Accuracy:  0.955, Loss:  0.039\n",
      "Epoch  90 Batch  867/1077 - Train Accuracy:  0.926, Validation Accuracy:  0.963, Loss:  0.080\n",
      " 91% ( 91/100) [########## ] eta 18:05 |\n",
      "Epoch  91 Batch   90/1077 - Train Accuracy:  0.965, Validation Accuracy:  0.967, Loss:  0.045\n",
      "Epoch  91 Batch  390/1077 - Train Accuracy:  0.936, Validation Accuracy:  0.951, Loss:  0.050\n",
      "Epoch  91 Batch  690/1077 - Train Accuracy:  0.958, Validation Accuracy:  0.969, Loss:  0.043\n",
      "Epoch  91 Batch  990/1077 - Train Accuracy:  0.986, Validation Accuracy:  0.972, Loss:  0.040\n",
      " 92% ( 92/100) [########## ] eta 16:03 /\n",
      "Epoch  92 Batch  213/1077 - Train Accuracy:  0.948, Validation Accuracy:  0.962, Loss:  0.033\n",
      "Epoch  92 Batch  513/1077 - Train Accuracy:  0.964, Validation Accuracy:  0.960, Loss:  0.052\n",
      "Epoch  92 Batch  813/1077 - Train Accuracy:  0.965, Validation Accuracy:  0.964, Loss:  0.042\n",
      " 93% ( 93/100) [########## ] eta 14:04 -\n",
      "Epoch  93 Batch   36/1077 - Train Accuracy:  0.973, Validation Accuracy:  0.969, Loss:  0.034\n",
      "Epoch  93 Batch  336/1077 - Train Accuracy:  0.952, Validation Accuracy:  0.957, Loss:  0.076\n",
      "Epoch  93 Batch  636/1077 - Train Accuracy:  0.979, Validation Accuracy:  0.971, Loss:  0.035\n",
      "Epoch  93 Batch  936/1077 - Train Accuracy:  0.975, Validation Accuracy:  0.960, Loss:  0.049\n",
      " 94% ( 94/100) [########## ] eta 12:03 \\\n",
      "Epoch  94 Batch  159/1077 - Train Accuracy:  0.979, Validation Accuracy:  0.964, Loss:  0.031\n",
      "Epoch  94 Batch  459/1077 - Train Accuracy:  0.974, Validation Accuracy:  0.960, Loss:  0.052\n",
      "Epoch  94 Batch  759/1077 - Train Accuracy:  0.963, Validation Accuracy:  0.956, Loss:  0.036\n",
      "Epoch  94 Batch 1059/1077 - Train Accuracy:  0.961, Validation Accuracy:  0.966, Loss:  0.041\n",
      " 95% ( 95/100) [########## ] eta 10:02 |\n",
      "Epoch  95 Batch  282/1077 - Train Accuracy:  0.943, Validation Accuracy:  0.958, Loss:  0.054\n",
      "Epoch  95 Batch  582/1077 - Train Accuracy:  0.974, Validation Accuracy:  0.971, Loss:  0.034\n",
      "Epoch  95 Batch  882/1077 - Train Accuracy:  0.980, Validation Accuracy:  0.957, Loss:  0.033\n",
      " 96% ( 96/100) [########## ] eta 08:03 /\n",
      "Epoch  96 Batch  105/1077 - Train Accuracy:  0.973, Validation Accuracy:  0.962, Loss:  0.032\n",
      "Epoch  96 Batch  405/1077 - Train Accuracy:  0.962, Validation Accuracy:  0.966, Loss:  0.045\n",
      "Epoch  96 Batch  705/1077 - Train Accuracy:  0.972, Validation Accuracy:  0.963, Loss:  0.049\n",
      "Epoch  96 Batch 1005/1077 - Train Accuracy:  0.971, Validation Accuracy:  0.959, Loss:  0.043\n",
      " 97% ( 97/100) [########## ] eta 06:02 -\n",
      "Epoch  97 Batch  228/1077 - Train Accuracy:  0.967, Validation Accuracy:  0.966, Loss:  0.027\n",
      "Epoch  97 Batch  528/1077 - Train Accuracy:  0.961, Validation Accuracy:  0.958, Loss:  0.036\n",
      "Epoch  97 Batch  828/1077 - Train Accuracy:  0.984, Validation Accuracy:  0.962, Loss:  0.032\n",
      " 98% ( 98/100) [########## ] eta 04:02 \\\n",
      "Epoch  98 Batch   51/1077 - Train Accuracy:  0.968, Validation Accuracy:  0.972, Loss:  0.036\n",
      "Epoch  98 Batch  351/1077 - Train Accuracy:  0.963, Validation Accuracy:  0.967, Loss:  0.038\n",
      "Epoch  98 Batch  651/1077 - Train Accuracy:  0.976, Validation Accuracy:  0.962, Loss:  0.039\n",
      "Epoch  98 Batch  951/1077 - Train Accuracy:  0.967, Validation Accuracy:  0.962, Loss:  0.045\n",
      " 99% ( 99/100) [########## ] eta 02:01 |\n",
      "Epoch  99 Batch  174/1077 - Train Accuracy:  0.987, Validation Accuracy:  0.964, Loss:  0.027\n",
      "Epoch  99 Batch  474/1077 - Train Accuracy:  0.957, Validation Accuracy:  0.963, Loss:  0.035\n",
      "Epoch  99 Batch  774/1077 - Train Accuracy:  0.973, Validation Accuracy:  0.959, Loss:  0.037\n",
      "Epoch  99 Batch 1074/1077 - Train Accuracy:  0.977, Validation Accuracy:  0.968, Loss:  0.042\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import time\n",
    "\n",
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1]), (0,0)],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, np.argmax(logits, 2)))\n",
    "\n",
    "train_source = source_int_text[batch_size:]\n",
    "train_target = target_int_text[batch_size:]\n",
    "\n",
    "valid_source = helper.pad_sentence_batch(source_int_text[:batch_size])\n",
    "valid_target = helper.pad_sentence_batch(target_int_text[:batch_size])\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch) in enumerate(\n",
    "                helper.batch_data(train_source, train_target, batch_size)):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 sequence_length: target_batch.shape[1],\n",
    "                 keep_prob: keep_probability})\n",
    "            \n",
    "\n",
    "            \n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(source_int_text) // batch_size + batch_i) % show_every_n_batches == 0:\n",
    "                if (last_epoch != epoch_i):\n",
    "                    last_epoch = epoch_i\n",
    "                    bar.numerator = last_epoch\n",
    "                    print(bar, end='\\n')\n",
    "                batch_train_logits = sess.run(\n",
    "                    inference_logits,\n",
    "                    {input_data: source_batch, keep_prob: 1.0})\n",
    "                batch_valid_logits = sess.run(\n",
    "                inference_logits,\n",
    "                {input_data: valid_source, keep_prob: 1.0})\n",
    "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
    "                valid_acc = get_accuracy(np.array(valid_target), batch_valid_logits)\n",
    "                end_time = time.time()\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.3f}, Validation Accuracy: {:>6.3f}, Loss: {:>6.3f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
    "                \n",
    "\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Save Parameters\n",
    "Save the `batch_size` and `save_path` parameters for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = helper.load_preprocess()\n",
    "load_path = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Sentence to Sequence\n",
    "To feed a sentence into the model for translation, you first need to preprocess it.  Implement the function `sentence_to_seq()` to preprocess new sentences.\n",
    "\n",
    "- Convert the sentence to lowercase\n",
    "- Convert words into ids using `vocab_to_int`\n",
    " - Convert words not in the vocabulary, to the `<UNK>` word id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_seq(sentence, vocab_to_int):\n",
    "    \"\"\"\n",
    "    Convert a sentence to a sequence of ids\n",
    "    :param sentence: String\n",
    "    :param vocab_to_int: Dictionary to go from the words to an id\n",
    "    :return: List of word ids\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return [vocab_to_int[word] if word in vocab_to_int else vocab_to_int['<UNK>'] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_sentence_to_seq(sentence_to_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Translate\n",
    "This will translate `translate_sentence` from English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "  Word Ids:      [69, 183, 104, 75, 166, 203, 150]\n",
      "  English Words: ['he', 'saw', 'a', 'old', 'yellow', 'truck', '.']\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [61, 51, 308, 224, 28, 198, 51, 72, 144, 190, 1]\n",
      "  French Words: ['parfois', 'juin', 'préférées', 'aimeraient', 'mangue', 'humide', 'juin', 'préféré.', 'avez', 'pousse', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "translate_sentence = 'he saw a old yellow truck .'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    translate_logits = sess.run(logits, {input_data: [translate_sentence], keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
    "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in np.argmax(translate_logits, 1)]))\n",
    "print('  French Words: {}'.format([target_int_to_vocab[i] for i in np.argmax(translate_logits, 1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Imperfect Translation\n",
    "You might notice that some sentences translate better than others.  Since the dataset you're using only has a vocabulary of 227 English words of the thousands that you use, you're only going to see good results using these words.  For this project, you don't need a perfect translation. However, if you want to create a better translation model, you'll need better data.\n",
    "\n",
    "You can train on the [WMT10 French-English corpus](http://www.statmt.org/wmt10/training-giga-fren.tar).  This dataset has more vocabulary and richer in topics discussed.  However, this will take you days to train, so make sure you've a GPU and the neural network is performing well on dataset we provided.  Just make sure you play with the WMT10 corpus after you've submitted this project.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_language_translation.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
